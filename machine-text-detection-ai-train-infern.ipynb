{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":63768,"databundleVersionId":6995834,"sourceType":"competition"}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Load Libraries","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\nfrom transformers import get_linear_schedule_with_warmup\nfrom torch.optim import AdamW\n\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom torch.nn import BCEWithLogitsLoss\nfrom tqdm import tqdm\n\ntqdm.pandas()\n\ntrain_df = pd.read_csv('/kaggle/input/artificial-text-detection-homework/dev.csv')\n\ntrain_X = train_df['Text']\ntrain_y = train_df['Class'].apply(lambda x: 1 if x == 'M' else 0)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-20T18:39:16.098960Z","iopub.execute_input":"2023-11-20T18:39:16.099499Z","iopub.status.idle":"2023-11-20T18:39:22.362710Z","shell.execute_reply.started":"2023-11-20T18:39:16.099472Z","shell.execute_reply":"2023-11-20T18:39:22.361939Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load The DistilBert for classification","metadata":{}},{"cell_type":"code","source":"# Load the DistilBert tokenizer\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n# Load the DistilBert model\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=1)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T18:39:22.376803Z","iopub.execute_input":"2023-11-20T18:39:22.377617Z","iopub.status.idle":"2023-11-20T18:39:26.067878Z","shell.execute_reply.started":"2023-11-20T18:39:22.377583Z","shell.execute_reply":"2023-11-20T18:39:26.066776Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01230417d9a24f43957bd2b3a80089a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"851f8299f0c54ed29ad7e7e2c26c82d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e79259b1050427f8c9bf7a5de2ef6c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"673173ab03c34c41992c88b71e572db6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13a89fa5146746d8a268b0b55fb10de0"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Prepare Data","metadata":{}},{"cell_type":"code","source":"# Assuming train_X and train_y are your features and labels\nX_train, X_val, y_train, y_val = train_test_split(train_X, train_y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T18:39:22.364561Z","iopub.execute_input":"2023-11-20T18:39:22.364917Z","iopub.status.idle":"2023-11-20T18:39:22.375653Z","shell.execute_reply.started":"2023-11-20T18:39:22.364884Z","shell.execute_reply":"2023-11-20T18:39:22.374844Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class SentimentDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        \n        encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding = 'max_length', truncation=True)\n        return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'labels': torch.tensor(label)}","metadata":{"execution":{"iopub.status.busy":"2023-11-20T18:39:26.070274Z","iopub.execute_input":"2023-11-20T18:39:26.070636Z","iopub.status.idle":"2023-11-20T18:39:26.080007Z","shell.execute_reply.started":"2023-11-20T18:39:26.070604Z","shell.execute_reply":"2023-11-20T18:39:26.079074Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Create the datasets\ntrain_dataset = SentimentDataset(X_train.to_list(), y_train.tolist(), tokenizer, 512)\nvalid_dataset = SentimentDataset(X_val.to_list(), y_val.to_list(), tokenizer, 512)\n\n# Create the dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T18:39:26.081792Z","iopub.execute_input":"2023-11-20T18:39:26.082194Z","iopub.status.idle":"2023-11-20T18:39:26.090662Z","shell.execute_reply.started":"2023-11-20T18:39:26.082161Z","shell.execute_reply":"2023-11-20T18:39:26.089842Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Compile The Model","metadata":{}},{"cell_type":"code","source":"# Set up the loss function and optimizer\nloss_fn = BCEWithLogitsLoss()\noptimizer = AdamW(model.parameters(), lr=1e-5)\n\n# Set up the learning rate scheduler\ntotal_steps = len(train_loader) * 10\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n\n# Training loop\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T18:39:26.091960Z","iopub.execute_input":"2023-11-20T18:39:26.092344Z","iopub.status.idle":"2023-11-20T18:39:29.248896Z","shell.execute_reply.started":"2023-11-20T18:39:26.092313Z","shell.execute_reply":"2023-11-20T18:39:29.248029Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0-5): 6 x TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=1, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Train (Finetune) DistilBertForClassification","metadata":{}},{"cell_type":"code","source":"max_epochs_without_improvement = 50\nearly_stopping_counter = 0\nbest_val_loss = float('inf')\n\nfor epoch in range(100):\n    model.train()\n    for batch in tqdm(train_loader):\n        optimizer.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device).float()\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n    model.eval()\n    for batch in valid_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device).float()\n        with torch.no_grad():\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        val_loss = outputs.loss\n\n    print(f\"Epoch: {epoch}, Loss:  {loss.item()}, Validation Loss:  {val_loss.item()}\")\n\n    # Early stopping\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        early_stopping_counter = 0\n        # Save the best model\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': loss,\n        }, \"best_model.pth\")\n    else:\n        early_stopping_counter += 1\n\n    if early_stopping_counter >= max_epochs_without_improvement:\n        print(f\"Early stopping at epoch {epoch} as validation loss did not improve for {max_epochs_without_improvement} consecutive epochs.\")\n        break","metadata":{"execution":{"iopub.status.busy":"2023-11-20T18:39:29.250145Z","iopub.execute_input":"2023-11-20T18:39:29.250446Z","iopub.status.idle":"2023-11-20T18:53:35.554275Z","shell.execute_reply.started":"2023-11-20T18:39:29.250421Z","shell.execute_reply":"2023-11-20T18:53:35.553260Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"100%|██████████| 100/100 [00:44<00:00,  2.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Loss:  0.06303806602954865, Validation Loss:  0.027110636234283447\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:42<00:00,  2.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Loss:  0.004777418449521065, Validation Loss:  0.01609455794095993\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:43<00:00,  2.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 2, Loss:  0.005320049822330475, Validation Loss:  0.008808588609099388\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:43<00:00,  2.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 3, Loss:  0.004376300144940615, Validation Loss:  0.0588519461452961\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:42<00:00,  2.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 4, Loss:  0.004900057800114155, Validation Loss:  0.0015644796658307314\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:43<00:00,  2.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 5, Loss:  0.008449956774711609, Validation Loss:  0.07390110194683075\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:42<00:00,  2.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 6, Loss:  0.002575397491455078, Validation Loss:  0.05028006434440613\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:42<00:00,  2.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 7, Loss:  0.0013817816507071257, Validation Loss:  0.0004626185691449791\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:42<00:00,  2.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 8, Loss:  0.0023199047427624464, Validation Loss:  0.0007691933424212039\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:42<00:00,  2.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 9, Loss:  0.0023950624745339155, Validation Loss:  0.0008488741586916149\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:43<00:00,  2.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 10, Loss:  0.0030481191352009773, Validation Loss:  0.0008826056146062911\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:43<00:00,  2.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 11, Loss:  0.009705883450806141, Validation Loss:  0.0017815993633121252\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:43<00:00,  2.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 12, Loss:  0.001971648773178458, Validation Loss:  0.010741619393229485\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:43<00:00,  2.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 13, Loss:  0.0032216475810855627, Validation Loss:  0.0009874040260910988\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:42<00:00,  2.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 14, Loss:  0.002611996605992317, Validation Loss:  0.0005999378627166152\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:43<00:00,  2.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 15, Loss:  0.0036652041599154472, Validation Loss:  0.014932917430996895\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:42<00:00,  2.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 16, Loss:  0.0028073368594050407, Validation Loss:  0.0012263451935723424\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:42<00:00,  2.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 17, Loss:  0.0022739688865840435, Validation Loss:  0.013907439075410366\nEarly stopping at epoch 17 as validation loss did not improve for 10 consecutive epochs.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Predict on test","metadata":{}},{"cell_type":"code","source":"model_name = 'distilbert-base-uncased'\nmodel_path = '/kaggle/working/best_model.pth'\n# Define the model architecture\nmodel = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels = 1)\n\n# Load the trained model weights\ncheckpoint = torch.load(model_path)\nmodel.load_state_dict(checkpoint['model_state_dict'], strict=False)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T19:11:37.380020Z","iopub.execute_input":"2023-11-20T19:11:37.380413Z","iopub.status.idle":"2023-11-20T19:11:38.866574Z","shell.execute_reply.started":"2023-11-20T19:11:37.380383Z","shell.execute_reply":"2023-11-20T19:11:38.865488Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"DistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0-5): 6 x TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=1, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"def model_predit(text): \n    text = tokenizer(text, return_tensors='pt', truncation=True, max_length=512, padding='max_length').to(device)\n#   get prediction\n    predicted = model(**text).logits.item()\n    \n    return 'M' if int(predicted) == 1 else 'H'","metadata":{"execution":{"iopub.status.busy":"2023-11-20T19:12:00.911600Z","iopub.execute_input":"2023-11-20T19:12:00.912079Z","iopub.status.idle":"2023-11-20T19:12:00.917493Z","shell.execute_reply.started":"2023-11-20T19:12:00.912034Z","shell.execute_reply":"2023-11-20T19:12:00.916605Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv('/kaggle/input/artificial-text-detection-homework/test.csv')\ndf_test['Class'] = df_test['Text'].progress_apply(model_predit)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T19:12:01.966965Z","iopub.execute_input":"2023-11-20T19:12:01.967814Z","iopub.status.idle":"2023-11-20T19:16:00.363194Z","shell.execute_reply.started":"2023-11-20T19:12:01.967782Z","shell.execute_reply":"2023-11-20T19:16:00.362289Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"100%|██████████| 20000/20000 [03:58<00:00, 83.97it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"submission_df = pd.DataFrame({'ID': df_test['ID'], 'Class': df_test['Class']})\nsubmission_df.to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T19:16:38.097815Z","iopub.execute_input":"2023-11-20T19:16:38.098189Z","iopub.status.idle":"2023-11-20T19:16:38.142918Z","shell.execute_reply.started":"2023-11-20T19:16:38.098160Z","shell.execute_reply":"2023-11-20T19:16:38.141991Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# Upvote and Comment if you like this notebook😉","metadata":{}}]}